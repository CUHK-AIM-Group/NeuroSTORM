<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="NeuroSTORM: A Deep Learning Platform for fMRI Analysis">
  <meta property="og:title" content="NeuroSTORM: A Deep Learning Platform for fMRI Analysis"/>
  <meta property="og:description" content="NeuroSTORM: A Deep Learning Platform for fMRI Analysis"/>
  <meta property="og:url" content="https://cuhk-aim-group.github.io/NeuroSTORM/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/icon.png" />
  <meta property="og:image:width" content="600"/>
  <meta property="og:image:height" content="600"/>


  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style>
    .content a {
      color: #1e90ff;   /* 你可以换成你喜欢的颜色，如 #e67e22, #007bff 等 */
      text-decoration: underline;
      transition: color 0.2s;
    }
    .content a:hover {
      color: #ff6600;   /* 鼠标悬停时的颜色 */
    }
    .carousel.results-carousel img {
      max-width: 800px;
      width: 750%;
      height: auto;
      display: block;
      margin: 0 auto;
    }
  </style>
  
  <title>NeuroSTORM</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="icon" type="image/png" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img src="static/images/icon.png" alt="NeuroSTORM Icon" style="height: 2.0em; vertical-align: middle; margin-right: 0.1em;">
            NeuroSTORM: A Deep Learning Platform for fMRI Analysis
          </h1>

          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Pre-print Link -->
                <span class="link-block">
                  <a href="https://www.researchsquare.com/article/rs-6728658/v1" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- You can use a document icon from FontAwesome, for example: -->
                      <i class="fas fa-file-alt"></i>
                    </span>
                    <span>Pre-print</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- HUggingface Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/zxcvb20001/NeuroSTORM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height:1.2em;vertical-align:middle;">
                    </span>
                    <span>HuggingFace</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/stage1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          NeuroSTORM provides preprocessing tools for both volume-based and ROI-based methods.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/stage2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          NeuroSTORM enables model training with customizable models and datasets.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/stage3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Users can add custom downstream tasks to NeuroSTORM.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">1. Develop Deep-learning Methods with NeuroSTORM</h2>
      <div class="content">

        <ol>
          <li>
            <strong>How to preprocess your own data with NeuroSTORM</strong>
            <ul>
              <li>
                <b>Pre-processing:</b> 
                Please make sure you have completed the primary preprocessing pipeline, such as 
                <a href="https://fsl.fmrib.ox.ac.uk/fsl/docs/#/" target="_blank">FSL</a>,
                <a href="https://fmriprep.org/en/stable/" target="_blank">fMRIPrep</a>,
                or the <a href="https://github.com/Washington-University/HCPpipelines" target="_blank">HCP pipeline</a>, 
                and your data is aligned to MNI152 space.
                You may also use our provided shell script for brain extraction:
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/blob/main/datasets/brain_extraction.sh" target="_blank">brain_extraction.sh</a>
                (based on FSL BET, please install 
                <a href="https://fsl.fmrib.ox.ac.uk/fsl/docs/#/" target="_blank">FSL tool</a> first).
                After running, brain mask files in <code>.nii.gz</code> format will be generated in the output directory.
              </li>
              <li>
                <b>Prepare 4D input:</b> 
                Use <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/blob/main/datasets/preprocessing_volume.py" target="_blank">preprocessing_volume.py</a> 
                to preprocess your data for model input. This tool supports bulk dataset processing, including background removal, resampling, Z-normalization, and saving frames as <code>.pt</code> files. If CPU is limited, preprocess in advance; if disk speed is the bottleneck, online preprocessing during training is an option.
              </li>
              <li>
                <b>Prepare 2D input:</b> 
                Use <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/blob/main/datasets/generate_roi_data_from_nii.py" target="_blank">generate_roi_data_from_nii.py</a>
                to convert 3D/4D data to 2D ROI-based data using available brain atlases. Multiple datasets and atlases are supported.
              </li>
            </ul>
          </li>

          <li>
            <strong>How to run existing methods on supported datasets</strong>
            <br>
            You can use our prepared scripts to quickly reproduce the experiments from the paper: 
            <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/tree/main/scripts" target="_blank">scripts</a>
          </li>

          <li>
            <strong>How to add new methods</strong>
            <ul>
              <li>
                Add your model definition in 
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/tree/main/models" target="_blank">models</a>,
                and add the task head in 
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/tree/main/models/heads" target="_blank">models/heads</a>.
              </li>
              <li>
                If additional inputs or outputs are needed, modify 
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/blob/main/models/lightning_model.py" target="_blank">lightning_model.py</a>.
              </li>
            </ul>
          </li>

          <li>
            <strong>How to adapt NeuroSTORM to new datasets</strong>
            <ul>
              <li>
                Please preprocess fMRI sequences and align to MNI152 space using tools like 
                <a href="https://fmriprep.org/en/stable/" target="_blank">fMRIPrep</a>,
                <a href="https://github.com/Washington-University/HCPpipelines" target="_blank">HCP pipelines</a>,
                or refer to 
                <a href="https://biobank.ctsu.ox.ac.uk/crystal/crystal/docs/brain_mri.pdf" target="_blank">UK Biobank MRI</a>.
              </li>
              <li>
                Add your dataset in 
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/blob/main/datasets/preprocessing_volume.py" target="_blank">preprocessing_volume.py</a>.
                Modify:
                <ul>
                  <li>The naming convention for Volume data, in <code>determine_subject_name</code> function</li>
                  <li>Choose the resize method: if similar to HCP-YA, use <code>select_middle_96</code>, otherwise use <code>resize_to_96</code></li>
                </ul>
              </li>
              <li>
                Initialize your dataset in 
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/blob/main/datasets/fmri_datasets.py" target="_blank">fmri_datasets.py</a>,
                and define loader in 
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/blob/main/utils/data_module.py" target="_blank">data_module.py</a>
              </li>
            </ul>
          </li>

          <li>
            <strong>How to add new tasks</strong>
            <ul>
              <li>
                Define the dataset label format in the <code>make_subject_dict</code> function from 
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/blob/main/utils/data_module.py" target="_blank">data_module.py</a>
              </li>
              <li>
                Set the task type by specifying <code>--downstream_task</code> in the script.
              </li>
              <li>
                Choose a classification or regression head. For custom tasks, add a new head definition in 
                <a href="https://github.com/CUHK-AIM-Group/NeuroSTORM/tree/main/models/heads" target="_blank">models/heads</a>
              </li>
            </ul>
          </li>

          <li>
            <strong>Related Links</strong>
            <ul>
              <li><a href="https://github.com/Transconnectome/SwiFT" target="_blank">SwiFT</a></li>
              <li><a href="https://github.com/LifangHe/BrainGNN_Pytorch" target="_blank">BrainGNN_Pytorch</a></li>
              <li><a href="https://github.com/MedARC-AI/MindEyeV2" target="_blank">MindEyeV2</a></li>
              <li><a href="https://fsl.fmrib.ox.ac.uk/fsl/docs" target="_blank">FSL Docs</a></li>
              <li><a href="https://github.com/Washington-University/HCPpipelines" target="_blank">HCP Pipelines</a></li>
              <li><a href="https://github.com/nipreps/fmriprep" target="_blank">fMRIPrep</a></li>
            </ul>
          </li>
        </ol>

      </div>
    </div>
  </div>
</section>

<!--End paper poster -->


<!-- Supported Dataset -->
<section class="hero is-white">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">2. Supported Datasets</h2>
      <div class="content">

        <p>
          NeuroSTORM supports a wide range of publicly available fMRI datasets for both pre-training and downstream analysis. The table below summarizes key characteristics of each dataset, including the subject number, male/female ratio, spatial resolution, TR, and official homepage.
        </p>

        <table class="table is-striped is-hoverable is-fullwidth" style="font-size:1em;">
          <thead>
            <tr>
              <th>Dataset Name (Abbreviation)</th>
              <th>Subjects<br/>(Male/Female)</th>
              <th>Spatial Resolution</th>
              <th>TR (ms)</th>
              <th>Homepage</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><b>UK Biobank (UKB)</b></td>
              <td>40,842<br>(17,720 / 23,122)</td>
              <td>2.4 × 2.4 × 2.4 mm³</td>
              <td>735</td>
              <td><a href="https://www.ukbiobank.ac.uk/" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>Adolescent Brain Cognitive Development (ABCD)</b></td>
              <td>9,448<br>(4,931 / 4,517)</td>
              <td>2.4 × 2.4 × 2.4 mm³</td>
              <td>800</td>
              <td><a href="https://abcdstudy.org/" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>Human Connectome Project - Young Adult (HCP-YA)</b></td>
              <td>1,206<br>(550 / 656)</td>
              <td>2 × 2 × 2 mm³</td>
              <td>720</td>
              <td><a href="https://www.humanconnectome.org/" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>Human Connectome Project - Aging (HCP-A)</b></td>
              <td>725<br>(319 / 406)</td>
              <td>2 × 2 × 2 mm³</td>
              <td>800</td>
              <td><a href="https://www.humanconnectome.org/study/hcp-lifespan-aging" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>Human Connectome Project - Development (HCP-D)</b></td>
              <td>652<br>(301 / 351)</td>
              <td>2 × 2 × 2 mm³</td>
              <td>800</td>
              <td><a href="https://www.humanconnectome.org/study/hcp-lifespan-development" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>Human Connectome Project - Early Psychosis (HCP-EP)</b></td>
              <td>252<br>(94 / 158)</td>
              <td>2 × 2 × 2 mm³</td>
              <td>800</td>
              <td><a href="https://www.humanconnectome.org/study/hcp-early-psychosis" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>ADHD-200 Sample (ADHD200)</b></td>
              <td>973<br>(600 / 373)</td>
              <td>3 × 3 × 4 mm³</td>
              <td>2000</td>
              <td><a href="http://fcon_1000.projects.nitrc.org/indi/adhd200/" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>Autism Brain Imaging Data Exchange (ABIDE)</b></td>
              <td>1,112<br>(948 / 164)</td>
              <td>3 × 3 × 3 mm³</td>
              <td>2000</td>
              <td><a href="http://fcon_1000.projects.nitrc.org/indi/abide/" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>UCLA Consortium for Neuropsychiatric Phenomics (UCLA)</b></td>
              <td>272<br>(222 / 50)</td>
              <td>3 × 3 × 4 mm³</td>
              <td>2000</td>
              <td><a href="https://openneuro.org/datasets/ds000030" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>Center for Biomedical Research Excellence (COBRE)</b></td>
              <td>173<br>(130 / 43)</td>
              <td>3.75 × 3.75 × 4.55 mm³</td>
              <td>2000</td>
              <td><a href="https://fcon_1000.projects.nitrc.org/indi/retro/cobre.html" target="_blank">Link</a></td>
            </tr>
            <tr>
              <td><b>Motor Neuron Disease fMRI Dataset (MND)</b></td>
              <td>59<br>(44 / 15)</td>
              <td>2.395 × 2.395 × 2.4 mm³</td>
              <td>2000</td>
              <td>--</td>
            </tr>
            <tr>
              <td><b>Transdiagnostic Connectome Project (TCP)</b></td>
              <td>245<br>(143 / 102)</td>
              <td>2 × 2 × 2 mm³</td>
              <td>800</td>
              <td><a href="https://openneuro.org/datasets/ds004215" target="_blank">Link</a></td>
            </tr>
          </tbody>
        </table>

        <h3>Dataset Descriptions</h3>
        <ul>
          <details>
            <summary>
              <b>UK Biobank (UKB):</b>
              A large-scale prospective study from the UK containing health, genetic, and neuroimaging data of over 40,000 middle-aged participants. fMRI is acquired at 2.4mm isotropic resolution (TR=735ms).
            </summary>
            <p>
              The UK Biobank (UKB) is one of the world's largest population-based health resource projects, comprising extensive genetic, clinical, lifestyle, and imaging data from over 500,000 participants, of which more than 40,000 have multimodal brain MRI—including both resting-state and task-based fMRI scans. Initiated between 2006 and 2010, UKB focuses on adults aged 40–69, with repeated imaging on a subset, enabling longitudinal analyses. fMRI data are acquired on Siemens Skyra 3T scanners, with a resolution of 2.4×2.4×2.4 mm³, and a fast TR of 735ms. The dataset includes both rsfMRI (6 min scan) and tfMRI (motor, emotion, social, gambling, and relational tasks), plus comprehensive demographic, cognitive, and health phenotypes. Standardized preprocessing pipelines (including motion correction, ICA-FIX denoising, and registration to MNI152) are publicly available. UKB is widely used for large-scale brain-behavior association studies, disease risk modeling, and neuroimaging foundation model pre-training.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>UK Biobank (UKB)</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI and tfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>Emotion, Gambling, Motor, Relational, Social</td></tr>
              <tr><th>Age Range</th><td>40-69 years</td></tr>
              <tr><th>Gender Ratio</th><td>~46% male, ~54% female</td></tr>
              <tr><th>Patient/Control</th><td>Population-based (includes healthy and various disease cases)</td></tr>
              <tr><th>Disease Types</th><td>Various (not a patient cohort, but disease info available)</td></tr>
              <tr><th>Used in Tasks</th><td>Pre-training, Task 1 (age/gender prediction)</td></tr>
              <tr><th>Sample Size</th><td>~40,842 with fMRI</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>Adolescent Brain Cognitive Development (ABCD):</b>
              Longitudinal neuroimaging of ~9,500 children in the US, with multimodal data and 2.4mm/800ms fMRI scans.
            </summary>
            <p>
              The ABCD Study is the largest long-term study of brain development and child/adolescent health in the US. It follows over 11,800 children (9–10 at baseline) through adolescence, with repeated multimodal MRI, cognitive, behavioral, genetic, and environmental data collection. Neuroimaging includes high-resolution rsfMRI (2.4mm³, TR=800ms) and tfMRI (emotional, reward, cognitive tasks), harmonized across 21 sites and 3 major scanner vendors. Imaging pipelines are derived from HCP preprocessing (motion correction, normalization, artifact removal), with ROI time-series available (e.g., Schaefer atlas). ABCD supports studies of typical development, neuropsychiatric risk, and gene–brain–behavior relationships.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>ABCD</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI and tfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>Emotional n-back, Reward, Stop-signal, Monetary Incentive Delay</td></tr>
              <tr><th>Age Range</th><td>9–13 years (at latest release)</td></tr>
              <tr><th>Gender Ratio</th><td>~52% male, ~48% female</td></tr>
              <tr><th>Patient/Control</th><td>Community sample (includes healthy and at-risk youth)</td></tr>
              <tr><th>Disease Types</th><td>Not specific, but behavioral/clinical phenotypes available</td></tr>
              <tr><th>Used in Tasks</th><td>Pre-training, Task 1 (age/gender)</td></tr>
              <tr><th>Sample Size</th><td>~9,448 with fMRI</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>Human Connectome Project – Young Adult (HCP-YA), Aging (HCP-A), Development (HCP-D):</b>
              Three high-resolution (2mm) public datasets for mapping brain structure and function across the lifespan (children, young adults, elderly).
            </summary>
            <p>
              The HCP is an NIH initiative to map human brain connectivity with unprecedented detail. Three major lifespan datasets are:
              <ul>
                <li><b>HCP-YA</b>: 1,206 healthy young adults (22–37y), scanned at 3T and 7T. Imaging includes rsfMRI (2mm³, TR=720ms; 1 hour per subject), tfMRI (7 tasks: working memory, emotion, language, motor, gambling, relational, social), and dMRI. Extensively preprocessed: motion correction, ICA-FIX, MNI registration, surface/volumetric data.</li>
                <li><b>HCP-A</b>: 725 adults aged 36–100, using similar MRI protocols. Focused on typical aging and age-related brain changes.</li>
                <li><b>HCP-D</b>: 652 children and adolescents (ages 5–21), using harmonized imaging, enables developmental connectomics.</li>
              </ul>
              All datasets include rich behavioral, cognitive, and demographic data. Used widely for benchmarking machine learning, connectomics, and lifespan brain research.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>HCP-YA, HCP-A, HCP-D</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI and tfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>Emotion, Gambling, Language, Motor, Relational, Social, Working Memory</td></tr>
              <tr><th>Age Range</th><td>HCP-YA: 22–37; HCP-A: 36–100; HCP-D: 5–21</td></tr>
              <tr><th>Gender Ratio</th><td>~47% male, ~53% female (YA); similar balance in others</td></tr>
              <tr><th>Patient/Control</th><td>Healthy volunteers</td></tr>
              <tr><th>Disease Types</th><td>None (controls only)</td></tr>
              <tr><th>Used in Tasks</th><td>Pre-training, Task 1 (age/gender), Task 2 (phenotype), Task 5 (tfMRI state classification)</td></tr>
              <tr><th>Sample Size</th><td>HCP-YA: 1,206; HCP-A: 725; HCP-D: 652</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>Human Connectome Project – Early Psychosis (HCP-EP):</b>
              fMRI/clinical data for early psychosis research (252 subjects), 2mm, TR=800ms.
            </summary>
            <p>
              The HCP-EP dataset focuses on individuals in the early phases (within 5 years) of psychotic disorders, including both affective and non-affective psychoses, and matched healthy controls. Participants (ages 16–35) are clinically characterized, with rsfMRI (2mm³, TR=800ms) and full neurocognitive/clinical assessments. Imaging is harmonized with HCP-Lifespan protocols (motion correction, ICA-FIX, MNI). The dataset supports studies of biomarkers and network changes in schizophrenia spectrum disorders and is a benchmark for disease diagnosis tasks.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>HCP-EP</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>None</td></tr>
              <tr><th>Age Range</th><td>16–35 years</td></tr>
              <tr><th>Gender Ratio</th><td>~42% male, ~58% female</td></tr>
              <tr><th>Patient/Control</th><td>Patients and controls</td></tr>
              <tr><th>Disease Types</th><td>Early psychosis (schizophrenia, schizoaffective, bipolar with psychosis)</td></tr>
              <tr><th>Used in Tasks</th><td>Task 3 (disease diagnosis)</td></tr>
              <tr><th>Sample Size</th><td>252 (57 affective psychosis, 127 non-affective psychosis, 68 controls)</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>ADHD-200 Sample (ADHD200):</b>
              Multi-site data of 973 children/adolescents, 3×3×4mm, TR=2000ms, focused on ADHD diagnosis.
            </summary>
            <p>
              The ADHD-200 Sample is a multi-center open dataset for ADHD biomarker discovery. It consists of 973 children and adolescents (ages 7–21) from 8 US and 4 Chinese sites, including both ADHD (combined, inattentive, hyperactive-impulsive) and typically developing controls. Imaging includes resting-state fMRI (3×3×4 mm³, TR=2s) and T1-weighted MRI. Phenotypic data covers diagnosis, ADHD subtype, IQ, age, sex, and clinical symptoms. Preprocessing pipelines (Athena, NIAK, others) are public, supporting motion correction, normalization, and ROI extraction. ADHD200 is widely used for benchmarking machine learning models for disease classification.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>ADHD-200</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>None</td></tr>
              <tr><th>Age Range</th><td>7–21 years</td></tr>
              <tr><th>Gender Ratio</th><td>~73% male, ~27% female</td></tr>
              <tr><th>Patient/Control</th><td>ADHD patients and controls</td></tr>
              <tr><th>Disease Types</th><td>Attention-Deficit/Hyperactivity Disorder (ADHD)</td></tr>
              <tr><th>Used in Tasks</th><td>Task 3 (disease diagnosis)</td></tr>
              <tr><th>Sample Size</th><td>973 (362 ADHD, 611 controls)</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>Autism Brain Imaging Data Exchange (ABIDE):</b>
              Aggregated from 17 sites, 1,112 subjects (948 males, 164 females) for ASD studies, 3mm, TR=2000ms.
            </summary>
            <p>
              ABIDE collates resting-state fMRI and anatomical MRI from 1,112 subjects (539 with Autism Spectrum Disorder, 573 controls), ages 7–64, across 17 international sites. Imaging protocols are heterogeneous (typical: 3mm³, TR=2s). Extensive phenotypic/clinical data are included, covering ASD diagnosis, IQ, and behavioral scales. Preprocessing (multiple pipelines) includes normalization, head motion correction, nuisance regression, registration, and ROI-based time series extraction. ABIDE is a benchmark for autism connectomics and machine learning-based disorder classification.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>ABIDE</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>None</td></tr>
              <tr><th>Age Range</th><td>7–64 years</td></tr>
              <tr><th>Gender Ratio</th><td>~85% male, ~15% female</td></tr>
              <tr><th>Patient/Control</th><td>ASD patients and controls</td></tr>
              <tr><th>Disease Types</th><td>Autism Spectrum Disorder (ASD)</td></tr>
              <tr><th>Used in Tasks</th><td>Task 3 (disease diagnosis)</td></tr>
              <tr><th>Sample Size</th><td>1,112 (539 ASD, 573 controls)</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>UCLA Consortium for Neuropsychiatric Phenomics (UCLA):</b>
              272 subjects (multi-diagnostic), 3×3×4mm, TR=2000ms.
            </summary>
            <p>
              The UCLA dataset comprises multimodal MRI and neuropsychological data for 272 adults (aged 21–50), including healthy controls and patients with schizophrenia, bipolar disorder, and ADHD. Resting-state and task-based fMRI (3×3×4 mm³, TR=2s) are included, with rich cognitive, behavioral, and clinical phenotype data. Imaging was acquired on Siemens Trio 3T scanners. Preprocessing includes motion correction, normalization, and ROI time series extraction. This dataset enables studies of transdiagnostic neural signatures and supports disease classification benchmarks.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>UCLA Phenomics</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI and tfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>Sternberg, Stroop, Stop-signal, Task-switching</td></tr>
              <tr><th>Age Range</th><td>21–50 years</td></tr>
              <tr><th>Gender Ratio</th><td>~46% male, ~54% female</td></tr>
              <tr><th>Patient/Control</th><td>Patients and controls</td></tr>
              <tr><th>Disease Types</th><td>Schizophrenia, Bipolar Disorder, ADHD</td></tr>
              <tr><th>Used in Tasks</th><td>Task 3 (disease diagnosis)</td></tr>
              <tr><th>Sample Size</th><td>272 (130 healthy, 72 schizophrenia, 35 bipolar, 35 ADHD)</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>Center for Biomedical Research Excellence (COBRE):</b>
              173 subjects (schizophrenia and controls), 3.75×3.75×4.55mm, TR=2000ms.
            </summary>
            <p>
              COBRE provides MRI data for 89 schizophrenia patients and 84 healthy controls (aged 18–65), recruited at a single US site. Imaging includes rsfMRI (3.75×3.75×4.55 mm³, TR=2s), T1 MRI, and clinical/behavioral measures. Preprocessing follows standard steps: motion correction, normalization, ROI time series extraction. This dataset is widely used for machine learning classification of schizophrenia and connectome analysis.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>COBRE</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>None</td></tr>
              <tr><th>Age Range</th><td>18–65 years</td></tr>
              <tr><th>Gender Ratio</th><td>~70% male, ~30% female</td></tr>
              <tr><th>Patient/Control</th><td>Schizophrenia patients and controls</td></tr>
              <tr><th>Disease Types</th><td>Schizophrenia</td></tr>
              <tr><th>Used in Tasks</th><td>Task 3 (disease diagnosis)</td></tr>
              <tr><th>Sample Size</th><td>173 (89 patients, 84 controls)</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>Motor Neuron Disease fMRI Dataset (MND):</b>
              59 participants (ALS and controls), 2.4mm, TR=2000ms, collected in Australia.
            </summary>
            <p>
              The MND dataset features anatomical and resting-state fMRI (2.395×2.395×2.4mm³, TR=2s) from 59 subjects (36 with Amyotrophic Lateral Sclerosis—ALS, 23 controls), acquired at Herston Imaging Research Facility in Australia using Siemens Prisma 3T scanners. Detailed motor, cognitive, and clinical characterization is included. Imaging data are preprocessed (motion correction, normalization). This dataset is suitable for studying motor system degeneration and machine learning-based diagnosis.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>MND (Motor Neuron Disease)</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>None</td></tr>
              <tr><th>Age Range</th><td>Mean ~57 years (range 30–80)</td></tr>
              <tr><th>Gender Ratio</th><td>44 male, 15 female</td></tr>
              <tr><th>Patient/Control</th><td>ALS patients and controls</td></tr>
              <tr><th>Disease Types</th><td>Amyotrophic Lateral Sclerosis (ALS)</td></tr>
              <tr><th>Used in Tasks</th><td>Task 3 (disease diagnosis)</td></tr>
              <tr><th>Sample Size</th><td>59 (36 ALS, 23 controls)</td></tr>
            </table>
          </details>

          <details>
            <summary>
              <b>Transdiagnostic Connectome Project (TCP):</b>
              245 subjects with multiple psychiatric diagnoses (2mm, TR=800ms), harmonized imaging.
            </summary>
            <p>
              The TCP dataset consists of 245 adults (aged 18–65) with a diverse range of psychiatric conditions (including mood, anxiety, and psychotic disorders), along with healthy controls, recruited at Yale and McLean (US). Resting-state fMRI (2mm³, TR=800ms) is harmonized across sites using Siemens Prisma scanners. All participants undergo the same comprehensive psychiatric diagnostic interviews (DSM-5), cognitive battery, and clinical assessments. Preprocessing mirrors HCP pipelines (motion correction, ICA-FIX, MNI registration, global signal regression), providing analysis-ready ROI-based functional connectivity and supporting transdiagnostic biomarker research.
            </p>
            <table border="1" cellpadding="4" style="border-collapse:collapse;">
              <tr><th>Dataset Name</th><td>TCP (Transdiagnostic Connectome Project)</td></tr>
              <tr><th>fMRI Types</th><td>rsfMRI</td></tr>
              <tr><th>tfMRI Tasks</th><td>None</td></tr>
              <tr><th>Age Range</th><td>18–65 years</td></tr>
              <tr><th>Gender Ratio</th><td>~54% female, ~46% male</td></tr>
              <tr><th>Patient/Control</th><td>Mixed: patients (multiple psychiatric diagnoses) and controls</td></tr>
              <tr><th>Disease Types</th><td>Major depressive disorder, generalized anxiety, bipolar, psychotic disorders, etc.</td></tr>
              <tr><th>Used in Tasks</th><td>Task 2 (phenotype prediction), Task 3 (disease diagnosis)</td></tr>
              <tr><th>Sample Size</th><td>245</td></tr>
            </table>
          </details>
        </ul>


        <div class="container" style="margin-top:2em; margin-bottom:2em;">
          <div class="columns is-vcentered is-desktop">
            <div class="column has-text-centered">
              <figure>
                <img src="static/images/data_sun.png" alt="Dataset distribution sunburst plot" style="max-width:100%; height:380px; object-fit:contain;"/>
                <figcaption style="margin-top:0.5em; font-size:1em; color:#666;">Dataset and sex distribution overview.</figcaption>
              </figure>
            </div>
            <div class="column has-text-centered">
              <figure>
                <img src="static/images/sankey.png" alt="Sankey diagram for NeuroSTORM dataset composition" style="max-width:100%; height:380px; object-fit:contain;"/>
                <figcaption style="margin-top:0.5em; font-size:1em; color:#666;">Sample composition in NeuroSTORM datasets.</figcaption>
              </figure>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End supported Dataset -->


<!-- Supported downstream task -->
<section class="hero is-white">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">3. Supported Tasks</h2>
      <div class="content">

        <h3>1. Age and Gender Prediction</h3>
        <p>
          <b>Task Description:</b><br>
          This task assesses the ability of models to predict basic demographic variables—chronological age and biological sex—using only resting-state fMRI (rsfMRI) sequences as input. The input is a preprocessed 4D rsfMRI sequence for each subject. The output is either a continuous age value (for regression) or a categorical label (male/female, for classification). Age and sex are fundamental variables correlated with brain structure and function, and accurate prediction indicates that brain representations capture meaningful demographic information. This task is widely used as a baseline for evaluating the generalizability and biological relevance of neural representations in fMRI analysis.
        </p>
        <details>
          <summary>How to use in NeuroSTORM</summary>
          <ul>
            <li>
              <b>Sex classification (reported):</b><br>
              <code>--downstream_task_id 1 --downstream_task_type classification --task_name sex</code>
            </li>
            <li>
              <b>Age prediction (regression):</b><br>
              <code>--downstream_task_id 1 --downstream_task_type regression --task_name age</code>
            </li>
          </ul>
        </details>

        <h3>2. Phenotype Prediction</h3>
        <p>
          <b>Task Description:</b><br>
          This task involves predicting quantitative or categorical phenotypic scores (such as cognitive, behavioral, or clinical measurements) from fMRI data. The input is a preprocessed fMRI sequence (rsfMRI or tfMRI) and the output is a continuous score (for regression) corresponding to the target phenotype. Example outputs include MMSE scores, PANSS scores, DASS measures, and other clinical or cognitive test results. This task provides a direct evaluation of how well neural representations capture individual differences in brain function related to cognition, emotion, or disease traits, and is critical for developing clinically useful neuroimaging biomarkers.
        </p>
        <details>
          <summary>How to use in NeuroSTORM</summary>
          <ul>
            <li>
              <b>Phenotype regression:</b><br>
              <code>--downstream_task_id 2 --downstream_task_type regression --task_name your_score_name</code>
              <br>
              Replace <code>your_score_name</code> with the phenotype to predict (e.g., <code>MMSE</code>, <code>PANSS_Positive</code>).
            </li>
          </ul>
        </details>

        <div class="container" style="margin-top:2em; margin-bottom:2em;">
          <div class="columns is-vcentered is-desktop">
            <div class="column has-text-centered">
              <figure>
                <img src="static/images/box1.png" alt="Phenotype label distributions in HCP-YA" style="max-width:100%; height:380px; object-fit:contain;"/>
                <figcaption style="margin-top:0.5em; font-size:1em; color:#666;">
                  Distribution of selected phenotype scores in the HCP-YA dataset.
                </figcaption>
              </figure>
            </div>
            <div class="column has-text-centered">
              <figure>
                <img src="static/images/box2.png" alt="Phenotype label distributions in TCP" style="max-width:100%; height:380px; object-fit:contain;"/>
                <figcaption style="margin-top:0.5em; font-size:1em; color:#666;">
                  Distribution of representative phenotype scores in the TCP dataset.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>

        <h3>3. Disease Diagnosis</h3>
        <p>
          <b>Task Description:</b><br>
          This task requires the model to assign each subject to a diagnostic category (e.g., healthy control, ADHD, schizophrenia, autism) based on their resting-state or task-based fMRI data. The input is a preprocessed fMRI sequence for each subject. The output is a categorical disease label. Disease diagnosis tasks are central for translational neuroimaging, as they test the capacity of models to extract pathological signatures from brain activity and are directly relevant to clinical decision support and biomarker discovery.
        </p>
        <details>
          <summary>How to use in NeuroSTORM</summary>
          <ul>
            <li>
              <b>Disease classification:</b><br>
              <code>--downstream_task_id 3 --downstream_task_type classification --task_name diagnosis</code>
            </li>
          </ul>
        </details>

        <h3>4. fMRI Retrieval</h3>
        <p>
          <b>Task Description:</b><br>
          This task evaluates the ability of models to align fMRI activation patterns with external semantic information, such as images or textual descriptions. The input can be an fMRI sequence (e.g., recorded while viewing images) or a semantic embedding (e.g., image features). The output is the retrieval of matching semantic content given fMRI input, or vice versa. This task is foundational for brain decoding, neural representation alignment, and cross-modal retrieval, and is crucial for understanding how brain activity encodes semantic information.
        </p>
        <p>
          <b>Experimental Procedure:</b><br>
          The fMRI retrieval experiment proceeds as follows:
          <ol>
            <li>
              <b>Test Sample Selection:</b>
              Randomly select 300 test samples from the NSD dataset. Each sample includes an fMRI sequence and its associated natural image.
            </li>
            <li>
              <b>Compute Image Embeddings:</b>
              For each image, compute its CLIP embedding to obtain a semantic feature representation.
            </li>
            <li>
              <b>Candidate Pool Creation:</b>
              For each test image, use its CLIP embedding to query the LAION-5B dataset, retrieving the top 16 most similar images as candidates.
            </li>
            <li>
              <b>fMRI Embedding Extraction:</b>
              Process each test fMRI sequence with the analysis model to obtain its embedding in the same feature space as CLIP.
            </li>
            <li>
              <b>Similarity Computation and Retrieval:</b>
              <ul>
                <li>Compute cosine similarity between each fMRI embedding and the 16 candidate CLIP image embeddings. Select the image with the highest similarity as the retrieval result (brain-to-image retrieval).</li>
                <li>For image-to-brain retrieval, use the image's CLIP embedding to retrieve the matching fMRI embedding from the pool of fMRI embeddings.</li>
              </ul>
            </li>
            <li>
              <b>Evaluation:</b>
              Calculate top-1, top-3, and top-5 retrieval accuracy. Repeat the retrieval process multiple times to ensure statistical robustness.
            </li>
          </ol>
        </p>
        <details>
          <summary>How to use in NeuroSTORM</summary>
          <p>Support for this task in NeuroSTORM will be released soon.</p>
        </details>


        <h3>5. Task-based fMRI State Classification</h3>
        <p>
          <b>Task Description:</b><br>
          This task involves classifying which cognitive state or task condition a subject is in, based on their task-based fMRI (tfMRI) sequence. The input is a preprocessed tfMRI sequence corresponding to a specific cognitive experiment (e.g., language, emotion, gambling task). The output is a label indicating the task condition or cognitive state. Accurate state classification demonstrates that the model captures functionally relevant brain activation patterns, and this task is a key benchmark for evaluating generalization and sensitivity to cognitive manipulations in fMRI analysis.
        </p>
        <details>
          <summary>How to use in NeuroSTORM</summary>
          <ul>
            <li>
              <b>State classification:</b><br>
              <code>--downstream_task_id 5 --downstream_task_type classification --task_name state_classification</code>
            </li>
          </ul>
        </details>


      </div>
    </div>
  </div>
</section>
<!-- End supported downstream task-->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Maintained by the <a href="https://www.ee.cuhk.edu.hk/~yxyuan/" target="_blank">AIM group</a> at the Chinese University of Hong Kong.
            <br> This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
